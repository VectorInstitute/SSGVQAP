{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm_notebook\n",
    "from random import randint\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_data(data_list, input_path): \n",
    "    '''\n",
    "    Inputs: \n",
    "        - data_list: json file paths\n",
    "        - input_path: input_path\n",
    "        \n",
    "    Output:\n",
    "        - dataframe containing: \n",
    "              'paper_id', \n",
    "              'titles', \n",
    "              'abstracts', \n",
    "              'introductions', \n",
    "              'conclusions', \n",
    "              'full_bodytext', \n",
    "              'bodysections',\n",
    "              'body_text_citations', \n",
    "              'context_title_list', \n",
    "              'cite_start', \n",
    "              'cite_end', \n",
    "              'cite_mark'\n",
    "    This function is used to parse json files to return the output elements\n",
    "    '''\n",
    "    \n",
    "    bibentries_title = []\n",
    "    bibentries_token = []\n",
    "    for json_file in range(0, len(data_list)):\n",
    "        bibentries_token.append(list(data_list[json_file]['bib_entries'].keys()))\n",
    "\n",
    "    for token_num, token_list in enumerate(bibentries_token):\n",
    "        bibentry_title = []\n",
    "        for token_len, token in enumerate(token_list):\n",
    "            bibentry_title.append(data_list[token_num]['bib_entries'][token]['title'])\n",
    "        bibentries_title.append(bibentry_title)\n",
    "        \n",
    "    titles = []\n",
    "    all_info = []\n",
    "    paper_id = []\n",
    "    search_abstracts = []\n",
    "    for json_file in range(0, len(data_list)):\n",
    "        paper_id.append(data_list[json_file]['paper_id'])\n",
    "        titles.append(data_list[json_file]['metadata']['title'])\n",
    "        all_info.append(data_list[json_file]['body_text'])\n",
    "        try:\n",
    "            search_abstracts.append(data_list[json_file]['abstract'])\n",
    "        except IndexError:\n",
    "            search_abstracts.append(None)\n",
    "        except KeyError:\n",
    "            search_abstracts.append(None)\n",
    "\n",
    "    abstracts = []\n",
    "    for texts in search_abstracts:\n",
    "        local_abstract = []\n",
    "        if texts is not None:\n",
    "            for num in range(0, len(texts)):\n",
    "                local_abstract.append(texts[num]['text'])\n",
    "        abstracts.append(' '.join(local_abstract))\n",
    "\n",
    "    bodysections = []\n",
    "    full_bodytext = []\n",
    "    introductions = []\n",
    "    conclusions = []\n",
    "    cite_tokens = []\n",
    "    cite_start = []\n",
    "    cite_end = []\n",
    "    cite_mark = []\n",
    "    \n",
    "    for text_info in all_info:\n",
    "        local_info = []\n",
    "        local_cite_token = []\n",
    "        local_cite_start = []\n",
    "        local_cite_end = []\n",
    "        local_cite_mark = []\n",
    "        local_introduction = []\n",
    "        local_conclusion = []\n",
    "\n",
    "        for info_len in range(0, len(text_info)):\n",
    "            if text_info[info_len]['section'] == 'Introduction':\n",
    "                local_introduction.append(text_info[info_len]['text'])\n",
    "            elif text_info[info_len]['section'] == 'Conclusion':\n",
    "                local_conclusion.append(text_info[info_len]['text'])\n",
    "            local_info.append(text_info[info_len]['text'])\n",
    "        for indices in text_info:\n",
    "            for cite_spans in indices['cite_spans']:\n",
    "                local_cite_token.append(cite_spans['ref_id'])\n",
    "                local_cite_start.append(cite_spans['start'])\n",
    "                local_cite_end.append(cite_spans['end'])\n",
    "                try:\n",
    "                    local_cite_mark.append(cite_spans['text'])\n",
    "                except KeyError:\n",
    "                    local_cite_mark.append(None)\n",
    "        introductions.append(''.join(local_introduction))\n",
    "        conclusions.append(''.join(local_conclusion))\n",
    "        full_bodytext.append(' '.join(local_info))\n",
    "        bodysections.append(local_info)\n",
    "        cite_tokens.append(local_cite_token)\n",
    "        cite_start.append(local_cite_start)\n",
    "        cite_end.append(local_cite_end)\n",
    "        cite_mark.append(local_cite_mark)\n",
    "\n",
    "    bib_dict_list = []\n",
    "    for bib_ref, bib_ttl in (zip(bibentries_token, bibentries_title)):\n",
    "        bib_dict = {}\n",
    "        for bib_bib_ref, bib_bib_ttl in zip(bib_ref, bib_ttl):\n",
    "            bib_dict[bib_bib_ref] = bib_bib_ttl\n",
    "        bib_dict_list.append(bib_dict)\n",
    "\n",
    "    context_title_list = []\n",
    "    for cite_val, bib_val in (zip(cite_tokens, bib_dict_list)):\n",
    "        cite_set = cite_val\n",
    "        bib_set = set(bib_val)\n",
    "        context_title_temp = []\n",
    "        for value in cite_set:\n",
    "            for val in bib_set:\n",
    "                if value == val:\n",
    "                    context_title_temp.append(bib_val[value])\n",
    "                elif value == None:\n",
    "                    context_title_temp.append(None)\n",
    "                    break\n",
    "        context_title_list.append(context_title_temp)\n",
    "        \n",
    "    \n",
    "    fields = {\n",
    "              'paper_id': paper_id[0], \n",
    "              'titles': titles[0], \n",
    "              'abstracts': abstracts[0], \n",
    "              'introductions': introductions[0], \n",
    "              'conclusions': conclusions[0], \n",
    "              'full_bodytext': full_bodytext[0], \n",
    "              'bodysections': bodysections[0],\n",
    "              'context_title_list': context_title_list[0], \n",
    "              'cite_start': cite_start[0], \n",
    "              'cite_end': cite_end[0], \n",
    "              'cite_mark': cite_mark[0]\n",
    "            }\n",
    "    return fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "877120\n"
     ]
    }
   ],
   "source": [
    "flat_context_title_list = []\n",
    "for i in cord_file['context_title_list']:\n",
    "    for j in i:\n",
    "        flat_context_title_list.append(j)\n",
    "\n",
    "print(len(set(flat_context_title_list)))  # todo: scrape pubmed for full articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/ /anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a2bafdf1c642099e9a67789ffd822a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rule_based_titles = []\n",
    "rule_based_abstracts = []\n",
    "rule_based_conclusions = []\n",
    "\n",
    "wh_titles = []\n",
    "q_titles = []\n",
    "do_does_titles = []\n",
    "\n",
    "wh_abstracts = []\n",
    "q_abstracts = []\n",
    "do_does_abstracts = []\n",
    "\n",
    "wh_conclusions = []\n",
    "q_conclusions = []\n",
    "do_does_conclusions = []\n",
    "\n",
    "for i, j, k in tqdm_notebook(zip(cord_file['titles'], cord_file['conclusions'], cord_file['abstracts'])):\n",
    "    if i.startswith('Who') or i.startswith('What') or i.startswith('Where') or i.startswith('When') or i.startswith('Why') or i.startswith('How') or i.startswith('Can ') or i.startswith('Could '):\n",
    "        wh_titles.append(i)\n",
    "        wh_abstracts.append(k)\n",
    "        wh_conclusions.append(j)\n",
    "        \n",
    "    if i.endswith('?'):\n",
    "        q_titles.append(i)\n",
    "        q_abstracts.append(k)\n",
    "        q_conclusions.append(j)\n",
    "        \n",
    "    if i.startswith('Do ') or i.startswith('Does '):\n",
    "        do_does_titles.append(i)\n",
    "        do_does_abstracts.append(k)\n",
    "        do_does_conclusions.append(j)\n",
    "\n",
    "rule_based_titles.extend(wh_titles)\n",
    "rule_based_titles.extend(do_does_titles)\n",
    "rule_based_titles.extend(q_titles)\n",
    "\n",
    "rule_based_abstracts.extend(wh_abstracts)\n",
    "rule_based_abstracts.extend(do_does_abstracts)\n",
    "rule_based_abstracts.extend(q_abstracts)\n",
    "\n",
    "rule_based_conclusions.extend(wh_conclusions)\n",
    "rule_based_conclusions.extend(do_does_conclusions)\n",
    "rule_based_conclusions.extend(q_conclusions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/ /anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486f0a7a8c924df5bca411a8a9a52b54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=877120.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rule_based_context_titles = []\n",
    "context_wh_titles = []\n",
    "context_q_titles = []\n",
    "context_do_does_titles = []\n",
    "for i in tqdm_notebook(list(set(flat_context_title_list))):\n",
    "    try:\n",
    "        if i.startswith('Who') or i.startswith('What') or i.startswith('Where') or i.startswith('When') or i.startswith('Why') or i.startswith('How'):\n",
    "            context_wh_titles.append(i)\n",
    "\n",
    "        if i.endswith('?'):\n",
    "            context_q_titles.append(i)\n",
    "\n",
    "        if i.startswith('Do ') or i.startswith('Does '):\n",
    "            context_do_does_titles.append(i)\n",
    "    except AttributeError:\n",
    "        continue\n",
    "\n",
    "rule_based_context_titles.extend(context_wh_titles)\n",
    "rule_based_context_titles.extend(context_q_titles)\n",
    "rule_based_context_titles.extend(context_do_does_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n",
      "380\n"
     ]
    }
   ],
   "source": [
    "str_list = list(filter(None, rule_based_conclusions))\n",
    "print(len(set(str_list)))\n",
    "\n",
    "str_list = list(filter(None, rule_based_abstracts))\n",
    "print(len(set(str_list)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "553\n"
     ]
    }
   ],
   "source": [
    "print(len(set(rule_based_titles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "sme_data = pd.DataFrame()\n",
    "sme_data['questions'] = rule_based_titles\n",
    "sme_data['abstracts'] = rule_based_abstracts\n",
    "sme_data['conclusions'] = rule_based_conclusions\n",
    "\n",
    "sme_data_cite = pd.DataFrame()\n",
    "sme_data_cite['questions'] = list(set(rule_based_context_titles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordPOSTagger\n",
    "stanford_dir = '/stanford-postagger-2018-10-16/'\n",
    "modelfile = stanford_dir + 'models/english-bidirectional-distsim.tagger'\n",
    "jarfile = stanford_dir + 'stanford-postagger-3.9.2.jar'\n",
    "\n",
    "st = StanfordPOSTagger(model_filename=modelfile, path_to_jar=jarfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_rule_based_titles = list(set(cord_file['titles']).difference(set(rule_based_titles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/ /anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7664856dd8d24600b910586edafc5909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading default properties from tagger /h/ /stanford-postagger-2018-10-16/models/english-bidirectional-distsim.tagger\n",
      "Loading POS tagger from /h/ /stanford-postagger-2018-10-16/models/english-bidirectional-distsim.tagger ... done [0.6 sec].\n",
      "Exception in thread \"main\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat edu.stanford.nlp.sequences.ExactBestSequenceFinder.bestSequence(ExactBestSequenceFinder.java:129)\n",
      "\tat edu.stanford.nlp.sequences.ExactBestSequenceFinder.bestSequence(ExactBestSequenceFinder.java:37)\n",
      "\tat edu.stanford.nlp.tagger.maxent.TestSentence.runTagInference(TestSentence.java:341)\n",
      "\tat edu.stanford.nlp.tagger.maxent.TestSentence.testTagInference(TestSentence.java:328)\n",
      "\tat edu.stanford.nlp.tagger.maxent.TestSentence.tagSentence(TestSentence.java:151)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.tagSentence(MaxentTagger.java:1052)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.tagCoreLabelsOrHasWords(MaxentTagger.java:1843)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.tagAndOutputSentence(MaxentTagger.java:1853)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1764)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1825)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1598)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1554)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.main(MaxentTagger.java:1897)\n",
      "\n",
      "Loading default properties from tagger /h/ /stanford-postagger-2018-10-16/models/english-bidirectional-distsim.tagger\n",
      "Loading POS tagger from /h/ /stanford-postagger-2018-10-16/models/english-bidirectional-distsim.tagger ... done [0.6 sec].\n",
      "Exception in thread \"main\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat edu.stanford.nlp.sequences.ExactBestSequenceFinder.bestSequence(ExactBestSequenceFinder.java:87)\n",
      "\tat edu.stanford.nlp.sequences.ExactBestSequenceFinder.bestSequence(ExactBestSequenceFinder.java:37)\n",
      "\tat edu.stanford.nlp.tagger.maxent.TestSentence.runTagInference(TestSentence.java:341)\n",
      "\tat edu.stanford.nlp.tagger.maxent.TestSentence.testTagInference(TestSentence.java:328)\n",
      "\tat edu.stanford.nlp.tagger.maxent.TestSentence.tagSentence(TestSentence.java:151)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.tagSentence(MaxentTagger.java:1052)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.tagCoreLabelsOrHasWords(MaxentTagger.java:1843)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.tagAndOutputSentence(MaxentTagger.java:1853)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1764)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1825)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1598)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1554)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.main(MaxentTagger.java:1897)\n",
      "\n",
      "Loading default properties from tagger /h/ /stanford-postagger-2018-10-16/models/english-bidirectional-distsim.tagger\n",
      "Loading POS tagger from /h/ /stanford-postagger-2018-10-16/models/english-bidirectional-distsim.tagger ... done [0.6 sec].\n",
      "Exception in thread \"main\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat edu.stanford.nlp.sequences.ExactBestSequenceFinder.bestSequence(ExactBestSequenceFinder.java:87)\n",
      "\tat edu.stanford.nlp.sequences.ExactBestSequenceFinder.bestSequence(ExactBestSequenceFinder.java:37)\n",
      "\tat edu.stanford.nlp.tagger.maxent.TestSentence.runTagInference(TestSentence.java:341)\n",
      "\tat edu.stanford.nlp.tagger.maxent.TestSentence.testTagInference(TestSentence.java:328)\n",
      "\tat edu.stanford.nlp.tagger.maxent.TestSentence.tagSentence(TestSentence.java:151)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.tagSentence(MaxentTagger.java:1052)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.tagCoreLabelsOrHasWords(MaxentTagger.java:1843)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.tagAndOutputSentence(MaxentTagger.java:1853)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1764)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1825)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1598)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.runTagger(MaxentTagger.java:1554)\n",
      "\tat edu.stanford.nlp.tagger.maxent.MaxentTagger.main(MaxentTagger.java:1897)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tagged_list = []\n",
    "for sent in tqdm_notebook(non_rule_based_titles):\n",
    "    try:\n",
    "        tagged_list.append(st.tag(sent.split()))\n",
    "    except:\n",
    "        tagged_list.append('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find NP-(VBP/VBZ) structured statement titles and convert by adding copulas (\"is\", \"are\") or auxiliary verbs (\"does\", \"do\")\n",
    "\n",
    "### generate \"yes\"/\"no\" answer according to negation status of VB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_lengths = []\n",
    "for i in pos_tagged:\n",
    "    pos_lengths.append(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_pos = []\n",
    "for val, length in zip(pos_tagged, pos_lengths):\n",
    "    local_pos = []\n",
    "    if length > 0:\n",
    "        for lengths in range(0, length):\n",
    "            local_pos.append(val[lengths][1])\n",
    "    entire_pos.append(local_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = pd.DataFrame()\n",
    "pos_df['pos_tags'] = pos_tagged\n",
    "pos_df['titles'] = non_rule_based_titles\n",
    "pos_df['pos_vals'] = entire_pos\n",
    "pos_df['pos_lengths'] = pos_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pos_df.merge(cord_file, how = 'inner', on = 'titles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos_tags' 'titles' 'pos_vals' 'pos_lengths' 'paper_id' 'abstracts'\n",
      " 'introductions' 'conclusions' 'full_bodytext' 'bodysections'\n",
      " 'context_title_list' 'cite_start' 'cite_end' 'cite_mark']\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df[merged_df.pos_lengths != 0].drop_duplicates(subset = 'titles' ,keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artificial = merged_df[['titles', 'pos_vals', 'pos_lengths', 'abstracts', 'conclusions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'NN': 116747, 'JJ': 56875, 'IN': 56712, 'NNP': 48998, 'NNS': 30362, 'DT': 18705, 'CC': 15581, 'FW': 9601, 'CD': 6044, 'VBG': 5904, 'VBN': 4424, 'VBZ': 3596, 'TO': 3158, 'NNPS': 2234, 'VB': 1621, 'RB': 1554, 'VBP': 1227, 'VBD': 683, 'PRP$': 677, 'SYM': 401, 'WDT': 202, 'JJR': 191, 'MD': 150, ':': 142, 'PRP': 116, 'WRB': 90, ',': 86, 'WP': 86, '.': 45, 'LS': 43, 'RBR': 33, 'JJS': 30, 'RP': 14, '#': 11, '$': 11, 'UH': 9, \"''\": 7, 'RBS': 5, 'EX': 4, 'WP$': 2, 'PDT': 2, 'POS': 2, '``': 1})\n"
     ]
    }
   ],
   "source": [
    "flat_pos = []\n",
    "for pos in df_artificial['pos_vals']:\n",
    "    for val in pos:\n",
    "        flat_pos.append(val)\n",
    "print(Counter(flat_pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "vb_tags = []\n",
    "vb_titles = []\n",
    "for tag, val in zip(df_artificial['pos_vals'], df_artificial['titles']):\n",
    "    if ('VBG' or 'VBP' or 'VBZ' or 'VBD' or 'VBN') in tag:\n",
    "        vb_tags.append(tag)\n",
    "        vb_titles.append(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# improve with better set of rules\n",
    "\n",
    "prepend_list = ['Do', 'Does', 'What', 'Where', 'When', 'Why', 'How', 'Can', 'Could']\n",
    "\n",
    "lower_case = lambda s: s[:1].lower() + s[1:] if s else ''\n",
    "\n",
    "vb_questions = []\n",
    "for title in vb_titles:\n",
    "    vb_questions.append(random.choice(prepend_list) + ' ' + lower_case(title) + '?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "vb_df = pd.DataFrame()\n",
    "vb_df['questions'] = vb_questions\n",
    "vb_df['titles'] = vb_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artificial_final = df_artificial.merge(vb_df, how = 'inner', on = 'titles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_artificial_final = df_artificial_final[['titles', 'questions', 'abstracts', 'conclusions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where pandemic H1N1 in Canada and the use of evidence in developing public health policies e A policy analysis?\n",
      "---\n",
      "When characterizing the Transmission Potential of Zoonotic Infections from Minor Outbreaks?\n",
      "---\n",
      "What open Access A quality assessment of genetic association studies supporting susceptibility and outcome in acute lung injury?\n",
      "---\n",
      "Does health behavior education, e-research and a (H1N1) influenza (Swine Flu): bridging the gap between intentions and health behavior change?\n",
      "---\n",
      "Do distinguishing Molecular Features and Clinical Characteristics of a Putative New Rhinovirus Species, Human Rhinovirus C (HRV C)?\n",
      "---\n",
      "Could supporting on-line material?\n",
      "---\n",
      "What development of a duplex real-time RT-qPCR assay to monitor genome replication, gene expression and gene insert stability during in vivo replication of a prototype live attenuated canine distemper virus vector encoding SIV gag?\n",
      "---\n",
      "Can connectivity analyses of bioenergetic changes in schizophrenia: Identification of novel treatments Running title: Bioenergetic function in schizophrenia?\n",
      "---\n",
      "Where supporting Information Fully Dried Two-Dimensional Paper Network for Enzymatically Enhanced Detection of Nucleic Acid Amplicons?\n",
      "---\n",
      "What nodeomics: Pathogen Detection in Vertebrate Lymph Nodes Using Meta-Transcriptomics?\n",
      "---\n",
      "Why comparison of viral replication and IFN response in alpaca and bovine cells following bovine viral diarrhea virus infection?\n",
      "---\n",
      "How tackling feline infectious peritonitis via reverse genetics?\n",
      "---\n",
      "Does identification of Neutralizing Monoclonal Antibodies Targeting Novel Conformational Epitopes of the Porcine Epidemic Diarrhoea Virus Spike Protein?\n",
      "---\n",
      "Does factors affecting cross-hospital exchange of Electronic Medical Records?\n",
      "---\n",
      "Where review Article · Übersichtsarbeit Pathogen Inactivation of Platelet and Plasma Blood Components for Transfusion Using the INTERCEPT Blood System™?\n",
      "---\n",
      "Can delivery System of CpG Oligodeoxynucleotides through Eliciting an Effective T cell Immune Response against Melanoma in Mice?\n",
      "---\n",
      "How molecular Sciences Editorial Molecular Research on Emerging Viruses: Evolution, Diagnostics, Pathogenesis, and Therapeutics?\n",
      "---\n",
      "Can nucleocapsid protein of porcine reproductive and respiratory syndrome virus antagonizes the antiviral activity of TRIM25 by interfering with TRIM25-mediated RIG-I ubiquitination?\n",
      "---\n",
      "Does novel and potent inhibitors targeting DHODH, a rate-limiting enzyme in de novo pyrimidine biosynthesis, are broad-spectrum antiviral against RNA viruses including newly emerged coronavirus SARS-CoV-2?\n",
      "---\n",
      "Where reducing the Impact of the Next Influenza Pandemic Using Household-Based Public Health Interventions A B S T R A C T Background?\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "rand_num = randint(0, len(vb_questions))\n",
    "for question in vb_questions[rand_num: rand_num+20]:\n",
    "    print(question)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "vb_df = pd.DataFrame()\n",
    "vb_df['questions'] = vb_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coala",
   "language": "python",
   "name": "coala"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
